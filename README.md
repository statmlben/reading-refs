# Reading references

*We are neither slave nor enemy of theory.*

Statistical learning theory
------
12-19 [Calculus of variation](https://www.reed.edu/physics/faculty/wheeler/documents/Classical%20Field%20Theory/Class%20Notes/Field%20Theory%20Chapter%205.pdf)

12-19 [On Methods of Sieves And Penalization](https://projecteuclid.org/download/pdf_1/euclid.aos/1030741085)

01-19 [Concentration inequalities and asymptotic results for ratio type empirical processes](https://projecteuclid.org/euclid.aop/1151418495)

06-19 [Benign Overfitting in Linear Regression](https://arxiv.org/abs/1906.11300)

01-20 [Fantastic Generalization Measures and Where to Find Them](https://arxiv.org/pdf/1912.02178.pdf)

01-20 [Boosted Kernel Ridge Regression: Optimal Learning Rates and Early Stopping](http://www.jmlr.org/papers/volume20/18-063/18-063.pdf)

01-20 [Tunability: Importance of Hyperparameters of Machine Learning Algorithms](http://www.jmlr.org/papers/volume20/18-444/18-444.pdf)

01-20 [Variance-based Regularization with Convex Objectives](http://www.jmlr.org/papers/volume20/17-750/17-750.pdf)

01-20 [Consistency of the MLE under mixture models](https://arxiv.org/pdf/1607.01251.pdf)

01-20 [Unbiased Generative Semi-Supervised Learning](http://jmlr.csail.mit.edu/papers/volume15/foxroberts14a/foxroberts14a.pdf)

01-20 [The Effect of Model Misspecification on Semi-Supervised Classification](https://ieeexplore.ieee.org/document/5728822)

02-20 [Variance of asymmetric U-statistic](https://www.stat.berkeley.edu/~bartlett/courses/2013spring-stat210b/notes/5notes.pdf)

02-20 [Invariance reduces Variance: Understanding Data Augmentation in Deep Learning and Beyond](https://arxiv.org/pdf/1907.10905.pdf)

02-20 [Small samples, and the margin of error by Terence Tao](https://terrytao.wordpress.com/2008/10/10/small-samples-and-the-margin-of-error/)

02-20 [Talagrandâ€™s concentration inequality by Terence Tao](https://terrytao.wordpress.com/2009/06/09/talagrands-concentration-inequality/)

03-20 [Inference for multivariate normal mixtures](https://www.sciencedirect.com/science/article/pii/S0047259X08002728)

03-20 [Triangle Array CLT](https://www.stat.berkeley.edu/users/pitman/s205f02/lecture10.pdf)

03-20 [A probability analysis on the value of unlabeled data for classification problems](http://tongzhang-ml.org/papers/icml00-unlabeled.pdf)

03-20 [On Generalization Bounds of a Family of Recurrent Neural Networks](https://arxiv.org/pdf/1910.12947.pdf)

03-20 [EM algorithm: Andrew Ng](http://cs229.stanford.edu/notes/cs229-notes8.pdf)


Methodology
------
12-19 [AR-Net: A simple Auto-Regressive Neural Network for time-series
](https://arxiv.org/pdf/1911.12436.pdf)

12-19 [Self-Taught Object Localization with Deep Networks](https://arxiv.org/pdf/1409.3964.pdf)

12-19 [Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks](https://arxiv.org/pdf/1809.03193.pdf)

09-19 [YOLOv3: An Incremental Improvement](https://arxiv.org/pdf/1804.02767.pdf)

09-19 [Learning Deep Features for Discriminative Localization](https://arxiv.org/abs/1512.04150)

09-19 [Are we really making much progress? A worrying analysis of recent neural recommendation approaches](https://dl.acm.org/doi/10.1145/3298689.3347058)

10-19 [Robust Bi-Tempered Logistic Loss Based on Bregman Divergences](https://arxiv.org/pdf/1906.03361.pdf)

12-19 [AP-Perf: Incorporating Generic Performance Metrics in Differentiable Learning](https://arxiv.org/pdf/1912.00965.pdf)

12-19 [Practical Bayesian Optimization of Machine Learning Algorithms](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)
- code: https://github.com/fmfn/BayesianOptimization

02-19 [Domain-Adversarial Training of Neural Networks](http://www.jmlr.org/papers/volume17/15-239/15-239.pdf)

01-20 [Distribution-Independent PAC Learning of Halfspaces with Massart Noise](https://arxiv.org/pdf/1906.10075.pdf)

01-20 [Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices](http://www.jmlr.org/papers/volume20/18-875/18-875.pdf)

01-20 [An Efficient Two Step Algorithm for High Dimensional
Change Point Regression Models Without Grid Search](http://www.jmlr.org/papers/volume20/18-460/18-460.pdf)

01-20 [Semi-Supervised Learning with Deep Generative Models
](https://arxiv.org/abs/1406.5298)

01-20 [Semi-supervised Learning by Entropy Minimization](http://papers.nips.cc/paper/2740-semi-supervised-learning-by-entropy-minimization.pdf)

01-20 [Reliable Decision Support using Counterfactual Models](https://arxiv.org/pdf/1703.10651.pdf)

01-20 [Improved Variational Inference with Inverse Autoregressive Flow](https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf)

01-20 [Learning to Classify Ordinal Data: The Data Replication Method](http://www.jmlr.org/papers/volume8/cardoso07a/cardoso07a.pdf)

01-20 [Classification with imperfect training labels](https://arxiv.org/pdf/1805.11505.pdf)

01-20 [Texygen: A Benchmarking Platform for Text Generation Models
](https://arxiv.org/abs/1802.01886)

02-20 [Multiclass Boosting: Margins, Codewords, Losses, and Algorithms
](http://www.jmlr.org/papers/volume20/17-137/17-137.pdf)

02-20 [Concavity of the Ordinal Log Likelihood](https://www.tandfonline.com/doi/abs/10.1080/01621459.1981.10477613)

02-20 [Variational Autoencoder](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)

02-20 [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)

03-20 [Darts: Differentiable architecture search](https://arxiv.org/pdf/1806.09055.pdf)

03-20 [LEARNING DEEP REPRESENTATIONS BY MUTUAL INFORMATION ESTIMATION AND MAXIMIZATION](https://arxiv.org/pdf/1808.06670.pdf)

03-20 [Improving Maximum Likelihood Training for Text Generation with Density Ratio Estimation](http://lantaoyu.com/files/aistats2020.pdf)

04-20 [Optimized Score Transformation for Fair Classification](https://arxiv.org/abs/1906.00066)

04-20 [Fair Decisions Despite Imperfect Predictions](https://arxiv.org/pdf/1902.02979.pdf)

04-20 [Identifying and Correcting Label Bias in Machine Learning](https://arxiv.org/pdf/1901.04966.pdf)

04-20 [Imputation estimators for unnormalized models with missing data](https://arxiv.org/pdf/1903.03630.pdf)

04-20 [Risk Bounds for Learning Multiple Components with Permutation-Invariant Losses](https://arxiv.org/pdf/1904.07594.pdf)

Optimization
--------

12-19 [Minimizing a Quadratic over A Sphere](http://users.clas.ufl.edu/hager/papers/Regular/sphere.pdf)

12-19 [Quadratic Programming Over Ellipsoids](https://arxiv.org/pdf/1711.04401.pdf)

01-20 [Non-Convex Matrix Completion and Related Problems via Strong Duality](http://www.jmlr.org/papers/volume20/17-611/17-611.pdf)

01-20 [Scalable Interpretable Multi-Response Regression via SEED](http://www.jmlr.org/papers/volume20/18-200/18-200.pdf)

01-20 [Optimization for deep learning: theory and algorithms](https://arxiv.org/pdf/1912.08957.pdf)

02-20 [Convergence Guarantees for a Class of Non-convex and Non-smooth Optimization Problems
](http://www.jmlr.org/papers/volume20/18-762/18-762.pdf)

02-20 [Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals](http://www.jmlr.org/papers/volume20/18-616/18-616.pdf)

02-20 [Gradient Descent Finds Global Minima of Deep Neural Networks](https://arxiv.org/pdf/1811.03804.pdf)

02-20 [Towards moderate overparameterization: global convergence guarantees for training shallow neural networks](https://arxiv.org/pdf/1902.04674.pdf)

02-20 [Global optimality conditions for deep neural networks](https://openreview.net/pdf?id=BJk7Gf-CZ)

02-20 [Target Propagation in Recurrent Neural Networks](http://jmlr.org/papers/volume21/18-141/18-141.pdf)

03-20 [Elimination of All Bad Local Minima in Deep Learning](https://arxiv.org/pdf/1901.00279.pdf)

04-20 [Ordered SGD: A New Stochastic Optimization Framework for Empirical Risk Minimization](https://arxiv.org/pdf/1907.04371.pdf)

Inference
--------
12-19 [Quadratic Form of Random Variable](http://pages.stat.wisc.edu/~st849-1/lectures/Ch02.pdf)

12-19 [The Likelihood Ratio Test in High-dimensional Logistic Regression is Asymptotically a Rescaled Chi-square](https://link.springer.com/content/pdf/10.1007%2Fs00440-018-00896-9.pdf)

12-19 [On High-dimensional Constrained Maximum Likelihood Inference](https://www.asc.ohio-state.edu/zhu.219/manuscript/inference.pdf)

01-20 [Fisher sharp null hypothesis](https://stats.stackexchange.com/questions/281200/is-fisher-sharp-null-hypothesis-testable)

01-20 [Double/debiased machine learning for treatment and structural parameters](https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097)

02-20 [Likelihood Ratio Tests for a Large Directed Acyclic Graph](https://www.tandfonline.com/doi/abs/10.1080/01621459.2019.1623042)

04-20 [LOCO: The Good, the Bad, and the Ugly](http://www.stat.cmu.edu/~ryantibs/talks/loco-2018.pdf)

04-20 [Distribution-Free Predictive Inference for Regression](https://www.stat.cmu.edu/~ryantibs/papers/conformal.pdf)

Coding
--------
12-19 [scikit-multilearn: A scikit-based Python environment for
performing multi-label classification](http://www.jmlr.org/papers/volume20/17-100/17-100.pdf)

12-19 [Fine-tuned BERT in Keras with Tensorflow hub](https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b)

12-19 [Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes](https://arxiv.org/pdf/1910.12478.pdf)

01-20 [PyOD: A Python Toolbox for Scalable Outlier Detection](http://www.jmlr.org/papers/volume20/19-011/19-011.pdf)

01-20 [imbalanced-learning](https://github.com/scikit-learn-contrib/imbalanced-learn#id23)

01-20 [Surpriselib: Python Library for Recommender System](http://surpriselib.com/)

02-20 [Modeling and Generating Random Vectors with Arbitrary Marginal Distributions and Correlation Matrix](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.48.281&rep=rep1&type=pdf)

02-20 [Python library: Keras GPT-2](https://github.com/CyberZHG/keras-gpt-2)

02-20 [Python library: gpt2-client](https://github.com/rish-16/gpt2client)

03-20 [Sparse_entropy_loss in Keras](https://github.com/Tony607/keras_sparse_categorical_crossentropy)


Deep Learning theory
---------
02-20 [Approximation and Estimation for High-Dimensional Deep Learning Networks](https://arxiv.org/pdf/1809.03090.pdf)

12-19 [Reconciling modern machine learning practice and the bias-variance trade-off](https://arxiv.org/abs/1812.11118)

12-19 [Deep Double Descent: Where Bigger Models and More Data Hurt](https://arxiv.org/pdf/1912.02292.pdf)

12-19 [How Much Over-parameterization Is Sufficient to Learn
Deep ReLU Networks?](https://arxiv.org/pdf/1911.12360.pdf)

01-20 [Understanding Deep Double Descent](https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent)

01-20 [Deep Optimal Stopping](http://www.jmlr.org/papers/volume20/18-232/18-232.pdf)

01-20 [Spectrally-normalized margin bounds for neural networks](https://arxiv.org/pdf/1706.08498.pdf)

01-20 [Error bounds for approximations with deep ReLU networks](https://arxiv.org/pdf/1610.01145.pdf)

01-20 [Surprises in High-Dimensional Ridgeless Least Squares Interpolation](https://arxiv.org/pdf/1903.08560.pdf)


Ranking
---------
01-20 [Spectral method and regularized MLE are both optimal for top-K ranking](https://projecteuclid.org/download/pdfview_1/euclid.aos/1558425643)

03-20 [A Multiclass Classification Approach to Label Ranking](https://arxiv.org/pdf/2002.09420.pdf)
